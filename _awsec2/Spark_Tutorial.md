---
published: true
title: "Apache Spark Tutorial (Step 3 on 4) "
collection: awsec2
layout: single
author_profile: true
---


* Spark can be configured with multiple cluster managers like YARN, Mesos etc. Along with that it can be configured in standalone mode.

** Standalone Deploy Mode :
Simplest way to deploy Spark on a private cluster. Both driver and worker nodes runs on the same machine.
Standalone mode is good to go for a developing applications in spark. Spark processes runs in JVM. Java should be pre-installed on the machines on which we have to run Spark job. ** 
